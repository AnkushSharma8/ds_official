
###Spark + ALS + daily embedding blending

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window
from pyspark.ml.feature import StringIndexer
from pyspark.ml.recommendation import ALS, ALSModel
from pyspark.sql.types import ArrayType, FloatType

spark = SparkSession.builder.getOrCreate()

DB = "cdp.product_recommendation"

# Tables
ALS_BACKBONE_TABLE = f"{DB}.als_backbone"
USER_IDX_MAP_TABLE = f"{DB}.user_idx_map"
ITEM_IDX_MAP_TABLE = f"{DB}.item_idx_map"
LONG_TERM_EMB_TABLE = f"{DB}.long_term_user_emb"
ITEM_EMB_TABLE = f"{DB}.item_emb"
FINAL_RECOS_TABLE = f"{DB}.final_recos"

# Paths
MODEL_PATH = "dbfs:/FileStore/models/als_monthly"

# Params
TOP_N = 100
RANK = 130
ALPHA_BLEND = 0.7  # weight for monthly ALS
BETA_BLEND = 0.3   # weight for daily behavior

---------------------------------------------------

###Train ALS → produce long-term user embeddings + item embeddings

def run_monthly(df_history):
    """
    Monthly ALS training on full historical data.
    Produces:
    - ALS model
    - User long-term embedding table
    - Item embedding table
    - Backbone (ALS-only) recommendations table
    """
    # Clean
    df = df_history.dropna(subset=["user_id", "skuCode", "total"])

    # Indexers
    user_indexer = StringIndexer(
        inputCol="user_id", outputCol="userIdx", handleInvalid="skip"
    ).fit(df)

    item_indexer = StringIndexer(
        inputCol="skuCode", outputCol="itemIdx", handleInvalid="skip"
    ).fit(df)

    df_idx = user_indexer.transform(df)
    df_idx = item_indexer.transform(df_idx)

    df_idx = df_idx.withColumn("userIdx", F.col("userIdx").cast("int"))
    df_idx = df_idx.withColumn("itemIdx", F.col("itemIdx").cast("int"))

    # Train ALS
    als = ALS(
        userCol="userIdx",
        itemCol="itemIdx",
        ratingCol="total",
        implicitPrefs=True,
        rank=RANK,
        regParam=3,
        alpha=15,
        nonnegative=True,
        coldStartStrategy="drop",
        maxIter=20
    )

    als_model = als.fit(df_idx)

    # Save ALS model
    als_model.write().overwrite().save(MODEL_PATH)

    # Save index maps
    df_idx.select("userIdx", "user_id").dropDuplicates(["userIdx"]).write.mode("overwrite").saveAsTable(USER_IDX_MAP_TABLE)
    df_idx.select("itemIdx", "skuCode").dropDuplicates(["itemIdx"]).write.mode("overwrite").saveAsTable(ITEM_IDX_MAP_TABLE)

    # Extract embeddings
    user_emb = als_model.userFactors.withColumnRenamed("id", "userIdx").withColumnRenamed("features", "user_emb")
    item_emb = als_model.itemFactors.withColumnRenamed("id", "itemIdx").withColumnRenamed("features", "item_emb")

    # Join with index→id mapping
    user_emb = user_emb.join(spark.table(USER_IDX_MAP_TABLE), "userIdx", "left").select("user_id", "user_emb")
    item_emb = item_emb.join(spark.table(ITEM_IDX_MAP_TABLE), "itemIdx", "left").select("skuCode", "item_emb")

    user_emb.write.mode("overwrite").saveAsTable(LONG_TERM_EMB_TABLE)
    item_emb.write.mode("overwrite").saveAsTable(ITEM_EMB_TABLE)

    # Backbone ALS-only recos
    recos = als_model.recommendForAllUsers(TOP_N)

    als_exp = (
        recos
        .select("userIdx", F.explode("recommendations").alias("rec"))
        .select("userIdx", F.col("rec.itemIdx").alias("itemIdx"), F.col("rec.rating").alias("als_score"))
        .join(spark.table(USER_IDX_MAP_TABLE), "userIdx")
        .join(spark.table(ITEM_IDX_MAP_TABLE), "itemIdx")
        .select("user_id", "skuCode", "als_score")
    )

    w = Window.partitionBy("user_id").orderBy(F.desc("als_score"))
    als_rank = als_exp.withColumn("rank", F.row_number().over(w)).filter(F.col("rank") <= TOP_N)

    als_rank.write.mode("overwrite").saveAsTable(ALS_BACKBONE_TABLE)

    return als_model
--------------------------------------


###Compute user short-term intent embedding

def run_daily_step1_short_embeddings(df_daily):
    """
    Compute short-term user embeddings from daily interactions.
    Output table: short_term_user_emb (temp view)
    """

    item_emb = spark.table(ITEM_EMB_TABLE)

    # Join with SKU embeddings
    df = (
        df_daily
        .dropna(subset=["user_id", "skuCode", "total"])
        .join(item_emb, "skuCode", "inner")
    )

    # Weighted average embedding UDF
    def weighted_avg(vectors, weights):
        import numpy as np
        return (np.array(vectors).T @ np.array(weights) / sum(weights)).tolist()

    wa_udf = F.udf(weighted_avg, ArrayType(FloatType()))

    grouped = (
        df.groupBy("user_id")
        .agg(
            F.collect_list("item_emb").alias("item_emb_list"),
            F.collect_list("total").alias("weight_list")
        )
        .withColumn("short_emb", wa_udf("item_emb_list", "weight_list"))
        .select("user_id", "short_emb")
    )

    grouped.createOrReplaceTempView("short_term_user_emb")

    return grouped
--------------------------------------------

####final_vec = α * long_term + (1−α) * short_term

def run_daily_step2_final_recos():
    """
    Blend long-term (ALS) embeddings + short-term embeddings.
    Generate top-N recommendations.
    """
    user_long = spark.table(LONG_TERM_EMB_TABLE)
    user_short = spark.table("short_term_user_emb")
    item_emb = spark.table(ITEM_EMB_TABLE)

    # Merge long-term + short-term vectors
    def blend_vec(long, short):
        import numpy as np
        if long is None:
            return short
        if short is None:
            return long
        return (ALPHA_BLEND * np.array(long) + BETA_BLEND * np.array(short)).tolist()

    blend_udf = F.udf(blend_vec, ArrayType(FloatType()))

    blended = (
        user_long.alias("l")
        .join(user_short.alias("s"), "user_id", "full")
        .withColumn("final_emb", blend_udf("l.user_emb", "s.short_emb"))
        .select("user_id", "final_emb")
    )

    # Cross join with items → compute dot product
    item_df = item_emb.select("skuCode", "item_emb")

    def dot(a, b):
        import numpy as np
        return float(np.dot(a, b))

    dot_udf = F.udf(dot, FloatType())

    scores = (
        blended.crossJoin(item_df)
        .withColumn("final_score", dot_udf("final_emb", "item_emb"))
        .select("user_id", "skuCode", "final_score")
    )

    # Rank top N per user
    w = Window.partitionBy("user_id").orderBy(F.desc("final_score"))

    ranked = (
        scores.withColumn("rank", F.row_number().over(w))
        .filter(F.col("rank") <= TOP_N)
    )

    ranked.write.mode("overwrite").saveAsTable(FINAL_RECOS_TABLE)

    return ranked
---------------------------------------------


def run_daily(df_daily):
    short = run_daily_step1_short_embeddings(df_daily)
    final = run_daily_step2_final_recos()
    return final

+++++++++++++++++++++++++++++++++++++++++++++++++


def run_daily_step2_blend():
    user_long = spark.table(LONG_TERM_EMB_TABLE)
    user_short = spark.table("short_term_user_emb")

    def blend(a, b):
        import numpy as np
        if a is None: return b
        if b is None: return a
        return (0.7 * np.array(a) + 0.3 * np.array(b)).tolist()

    blend_udf = F.udf(blend, ArrayType(FloatType()))

    blended = (
        user_long.alias("l")
        .join(user_short.alias("s"), "user_id", "outer")
        .withColumn("final_emb", blend_udf("l.user_emb", "s.short_emb"))
        .select("user_id", "final_emb")
    )

    return blended
-------



def run_daily_step3_rank(blended):
    item_emb = spark.table(ITEM_EMB_TABLE)
    als_candidates = spark.table(ALS_BACKBONE_TABLE)

    # Attach item vectors to ALS candidates
    candidates = (
        als_candidates
        .join(blended, "user_id")
        .join(item_emb, "skuCode")
    )

    # Dot product (final embedding · item embedding)
    def dot(a, b):
        import numpy as np
        return float(np.dot(a, b))

    dot_udf = F.udf(dot, FloatType())

    scored = candidates.withColumn(
        "final_score",
        dot_udf("final_emb", "item_emb")
    )

    # TOP 100 final recos
    w = Window.partitionBy("user_id").orderBy(F.desc("final_score"))

    final = (
        scored
        .withColumn("rank", F.row_number().over(w))
        .filter(F.col("rank") <= 100)
        .select("user_id", "skuCode", "final_score")
    )

    final.write.mode("overwrite").saveAsTable(FINAL_RECOS_TABLE)

    return final
-------------


def run_daily(df_daily):
    short = run_daily_step1_short_embeddings(df_daily)
    blended = run_daily_step2_blend()
    final = run_daily_step3_rank(blended)
    return final

