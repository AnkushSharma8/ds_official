# =====================================================
# 1. ALS Training & Recommendations
# =====================================================
from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer
from pyspark.ml.recommendation import ALS

# Prepare ALS input
df_als_prep = df_final.select("user_id", "skuCode", "total").dropna(subset=["user_id", "skuCode"])

# Index user_id and skuCode
user_indexer = StringIndexer(inputCol="user_id", outputCol="userIdx", handleInvalid="skip")
item_indexer = StringIndexer(inputCol="skuCode", outputCol="itemIdx", handleInvalid="skip")

ui_model = user_indexer.fit(df_als_prep)
ii_model = item_indexer.fit(df_als_prep)

df_als_indexed = ui_model.transform(df_als_prep)
df_als_indexed = ii_model.transform(df_als_indexed)

df_als_indexed = (
    df_als_indexed
    .withColumn("userIdx", F.col("userIdx").cast("int"))
    .withColumn("itemIdx", F.col("itemIdx").cast("int"))
)

# Train ALS model
als = ALS(
    userCol="userIdx", itemCol="itemIdx", ratingCol="total",
    implicitPrefs=True, rank=70, regParam=0.05, alpha=40,
    coldStartStrategy="drop", nonnegative=True, maxIter=15
)
als_model = als.fit(df_als_indexed)

# Mapping for decoding
item_idx_map = df_als_indexed.select("itemIdx", "skuCode").dropDuplicates(["itemIdx"])
user_idx_map = df_als_indexed.select("userIdx", "user_id").dropDuplicates(["userIdx"])

# Get Top-N recommendations
als_user_recs = als_model.recommendForAllUsers(50)

# Explode into flat table
als_exp = (
    als_user_recs
    .select("userIdx", F.explode("recommendations").alias("rec"))
    .select("userIdx", F.col("rec.itemIdx").alias("itemIdx"), F.col("rec.rating").alias("als_score"))
    .join(user_idx_map, on="userIdx", how="left")
    .join(item_idx_map, on="itemIdx", how="left")
    .select("user_id", "skuCode", "als_score")
)

# Enrich with metadata
als_enriched = (
    als_exp
    .join(df_category, on="skuCode", how="left")
    .select("user_id", "skuCode", "als_score", "skuName", "skuDescription", "medianRsp")
)

# =====================================================
# 2. Cosine Similarity + Conditional Filtering
# =====================================================
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.sql.types import DoubleType
import numpy as np

# Cosine similarity function
def cosine_similarity(v1, v2):
    v1 = np.array(v1.toArray())
    v2 = np.array(v2.toArray())
    denom = (np.linalg.norm(v1) * np.linalg.norm(v2))
    return float(np.dot(v1, v2) / denom) if denom != 0 else 0.0

# Main function
def get_filtered_recommendations(user_id, current_sku, top_n=50, min_recs=5, cosine_thresh=0.7):
    # 1. ALS recommendations for user
    als_recs_user = als_enriched.filter(F.col("user_id") == user_id).limit(top_n)
    if als_recs_user.count() == 0:
        raise ValueError(f"No ALS recommendations found for user {user_id}")

    # 2. Metadata for current intent SKU
    current_sku_meta = df_category.filter(F.col("skuCode") == current_sku)
    if current_sku_meta.count() == 0:
        raise ValueError(f"Current SKU {current_sku} not found in metadata.")

    # 3. Merge ALS recs + current SKU into one small DF
    combined_df = als_recs_user.select(
        "skuCode", "als_score", "skuName", "skuDescription", "medianRsp"
    ).unionByName(
        current_sku_meta.selectExpr(
            "skuCode", "cast(null as double) as als_score", "skuName", "skuDescription", "medianRsp"
        )
    )

    # 4. Build text features (EXCLUDE brandName & categoryName)
    combined_df = combined_df.withColumn(
        "text_features", 
        F.concat_ws(" ", "skuName", "skuDescription", F.col("medianRsp").cast("string"))
    )

    # TF-IDF pipeline
    tokenizer = Tokenizer(inputCol="text_features", outputCol="tokens")
    remover = StopWordsRemover(inputCol="tokens", outputCol="filtered_tokens")
    hashingTF = HashingTF(inputCol="filtered_tokens", outputCol="rawFeatures", numFeatures=5000)
    idf = IDF(inputCol="rawFeatures", outputCol="tfidfFeatures")

    df_tokens = tokenizer.transform(combined_df)
    df_removed = remover.transform(df_tokens)
    df_tf = hashingTF.transform(df_removed)
    idf_model = idf.fit(df_tf)
    df_tfidf = idf_model.transform(df_tf).select(
        "skuCode", "als_score", "skuName", "skuDescription", "medianRsp", "tfidfFeatures"
    )

    # 5. Extract current SKU vector
    current_vec_row = df_tfidf.filter(F.col("skuCode") == current_sku).select("tfidfFeatures").collect()
    current_vec = current_vec_row[0][0]

    # 6. Compute cosine similarity for ALS recs
    def sim_udf(v):
        return cosine_similarity(current_vec, v)

    sim_udf_pyspark = F.udf(sim_udf, DoubleType())

    result = (
        df_tfidf.filter(F.col("skuCode") != current_sku)
        .withColumn("cosine_similarity", sim_udf_pyspark("tfidfFeatures"))
    )

    # 7. Apply cosine similarity threshold
    filtered = result.filter(F.col("cosine_similarity") >= cosine_thresh)

    # 8. Backfill if filtered < min_recs
    count_filtered = filtered.count()
    if count_filtered < min_recs:
        missing = min_recs - count_filtered
        # Compute similarity with entire catalog
        df_cat = df_category.withColumn(
            "text_features", F.concat_ws(" ", "skuName", "skuDescription", F.col("medianRsp").cast("string"))
        )
        df_tokens_cat = tokenizer.transform(df_cat)
        df_removed_cat = remover.transform(df_tokens_cat)
        df_tf_cat = hashingTF.transform(df_removed_cat)
        df_tfidf_cat = idf_model.transform(df_tf_cat).select(
            "skuCode", "skuName", "skuDescription", "medianRsp", "tfidfFeatures"
        )
        df_tfidf_cat = df_tfidf_cat.filter(~F.col("skuCode").isin([r["skuCode"] for r in filtered.collect()] + [current_sku]))
        df_tfidf_cat = df_tfidf_cat.withColumn("cosine_similarity", sim_udf_pyspark("tfidfFeatures"))
        backfill = df_tfidf_cat.orderBy(F.col("cosine_similarity").desc()).limit(missing)
        filtered = filtered.unionByName(backfill)

    # 9. Final output sorted by cosine similarity and ALS score
    final_result = filtered.orderBy(F.col("cosine_similarity").desc(), F.col("als_score").desc())
    return final_result.select("skuCode", "skuName", "skuDescription", "medianRsp", "als_score", "cosine_similarity")

# =====================================================
# 3. Example Run
# =====================================================
user_id = "USER123"
current_sku = "SKU12345"

result = get_filtered_recommendations(user_id, current_sku, top_n=50, min_recs=5, cosine_thresh=0.7)
result.show(20, truncate=False)
