from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml.feature import StringIndexer
from pyspark.ml.recommendation import ALS

# =====================================================
# 1. ALS Training
# =====================================================
df_als_prep = df_final.select("user_id", "skuCode", "total") \
    .dropna(subset=["user_id", "skuCode"])

# Index user_id and skuCode
user_indexer = StringIndexer(inputCol="user_id", outputCol="userIdx", handleInvalid="skip")
item_indexer = StringIndexer(inputCol="skuCode", outputCol="itemIdx", handleInvalid="skip")

ui_model = user_indexer.fit(df_als_prep)
ii_model = item_indexer.fit(df_als_prep)

df_als_indexed = ui_model.transform(df_als_prep)
df_als_indexed = ii_model.transform(df_als_indexed)

df_als_indexed = (df_als_indexed
    .withColumn("userIdx", F.col("userIdx").cast("int"))
    .withColumn("itemIdx", F.col("itemIdx").cast("int"))
)

# Train ALS model
als = ALS(
    userCol="userIdx",
    itemCol="itemIdx",
    ratingCol="total",
    implicitPrefs=True,
    rank=70,
    regParam=0.05,
    alpha=40,
    coldStartStrategy="drop",
    nonnegative=True,
    maxIter=15
)

als_model = als.fit(df_als_indexed)

# Mapping for decoding
item_idx_map = df_als_indexed.select("itemIdx", "skuCode").dropDuplicates(["itemIdx"])
user_idx_map = df_als_indexed.select("userIdx", "user_id").dropDuplicates(["userIdx"])

# =====================================================
# 2. ALS Recommendations
# =====================================================
als_user_recs = als_model.recommendForAllUsers(200)  # Top 200 to be safe

als_exp = (
    als_user_recs
    .select("userIdx", F.explode("recommendations").alias("rec"))
    .select("userIdx", F.col("rec.itemIdx").alias("itemIdx"), F.col("rec.rating").alias("als_score"))
    .join(user_idx_map, on="userIdx", how="left")
    .join(item_idx_map, on="itemIdx", how="left")
    .select("user_id", "skuCode", "als_score")
)

# Enrich with metadata
als_enriched = (
    als_exp.join(df_category, on="skuCode", how="left")
    .select("user_id", "skuCode", "als_score", "brandName", "categoryName", "subCategoryName", "sku_description", "median_rsp")
)

# Add batch_date for retraining cycles
als_enriched = als_enriched.withColumn("batch_date", F.current_date())

# =====================================================
# 3. Raw Top-N (per user)
# =====================================================
window_raw = Window.partitionBy("user_id").orderBy(F.col("als_score").desc())
raw_topn = (
    als_enriched.withColumn("rank", F.row_number().over(window_raw))
    .filter(F.col("rank") <= 50)  # top_n = 50
    .drop("rank")
)

# Save table
raw_topn.write.mode("overwrite").format("delta").saveAsTable("recommendations.raw_topn")

# =====================================================
# 4. Subcategory Top-K (per user, per subcategory)
# =====================================================
window_subcat = Window.partitionBy("user_id", "subCategoryName").orderBy(F.col("als_score").desc())
subcat_topk = (
    als_enriched.withColumn("rank", F.row_number().over(window_subcat))
    .filter(F.col("rank") <= 10)  # top_k = 10
    .drop("rank")
)

subcat_topk.write.mode("overwrite").format("delta").saveAsTable("recommendations.subcat_topk")

# =====================================================
# 5. Combined Recommendations
# =====================================================
combined_recs = (
    raw_topn.unionByName(subcat_topk)
    .dropDuplicates(["user_id", "skuCode"])
)

combined_recs.write.mode("overwrite").format("delta").saveAsTable("recommendations.combined")
