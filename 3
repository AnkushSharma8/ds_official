from pyspark.sql import SparkSession, functions as F
from pyspark.ml.recommendation import ALS
from pyspark.ml.feature import StringIndexer, Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler
from pyspark.sql.functions import col, udf, explode
from pyspark.sql.types import DoubleType
from pyspark.sql.window import Window
import numpy as np

# -------------------------
# Spark Session
# -------------------------
spark = SparkSession.builder.appName("HybridRecPipeline").getOrCreate()

# -------------------------
# STEP 1: Prepare ALS Input
# -------------------------
df_als_prep = df_final.select("user_id", "skuCode", "total").dropna(subset=["user_id", "skuCode"])

# Index users and items
user_indexer = StringIndexer(inputCol="user_id", outputCol="userIdx", handleInvalid="skip")
item_indexer = StringIndexer(inputCol="skuCode", outputCol="itemIdx", handleInvalid="skip")
ui_model = user_indexer.fit(df_als_prep)
ii_model = item_indexer.fit(df_als_prep)

df_als_indexed = ui_model.transform(df_als_prep)
df_als_indexed = ii_model.transform(df_als_indexed)
df_als_indexed = df_als_indexed.withColumn("userIdx", col("userIdx").cast("int")) \
                               .withColumn("itemIdx", col("itemIdx").cast("int"))

# -------------------------
# STEP 2: Train ALS Model
# -------------------------
als = ALS(
    userCol="userIdx", itemCol="itemIdx", ratingCol="total",
    implicitPrefs=True, rank=70, regParam=0.05, alpha=40,
    coldStartStrategy="drop", nonnegative=True, maxIter=15
)
als_model = als.fit(df_als_indexed)

# -------------------------
# STEP 3: Generate ALS Recommendations
# -------------------------
item_idx_map = df_als_indexed.select("itemIdx", "skuCode").dropDuplicates(["itemIdx"])
user_idx_map = df_als_indexed.select("userIdx", "user_id").dropDuplicates(["userIdx"])

als_user_recs = als_model.recommendForAllUsers(50)

als_exp = (
    als_user_recs
    .select("userIdx", explode("recommendations").alias("rec"))
    .select("userIdx", col("rec.itemIdx").alias("itemIdx"), col("rec.rating").alias("als_score"))
    .join(user_idx_map, on="userIdx", how="left")
    .join(item_idx_map, on="itemIdx", how="left")
    .select("user_id", "skuCode", "als_score")
)

# Enrich ALS recs with category info
als_enriched = (
    als_exp.join(df_category, on="skuCode", how="left")
    .select("user_id", "skuCode", "als_score", "brandName", "categoryName", "skuDescription", "medianRsp")
)

# -------------------------
# STEP 4: Build Content Vectors (TF-IDF + Price)
# -------------------------
df_category_cb = df_category.withColumn(
    "text_features",
    F.concat_ws(" ", "brandName", "categoryName", "skuDescription")
)

# Tokenize
tokenizer = Tokenizer(inputCol="text_features", outputCol="tokens")
df_tokens = tokenizer.transform(df_category_cb)

# Remove stopwords
remover = StopWordsRemover(inputCol="tokens", outputCol="filtered_tokens")
df_removed = remover.transform(df_tokens)

# TF
hashingTF = HashingTF(inputCol="filtered_tokens", outputCol="rawFeatures", numFeatures=5000)
df_tf = hashingTF.transform(df_removed)

# IDF
idf = IDF(inputCol="rawFeatures", outputCol="tfidfFeatures")
idf_model = idf.fit(df_tf)
df_tfidf = idf_model.transform(df_tf)

# Final vector: price + text embedding
assembler = VectorAssembler(inputCols=["medianRsp", "tfidfFeatures"], outputCol="finalFeatures")
df_features = assembler.transform(df_tfidf).select("skuCode", "finalFeatures")

# -------------------------
# STEP 5: Cosine Similarity Function
# -------------------------
def cosine_similarity(v1, v2):
    v1 = np.array(v1.toArray())
    v2 = np.array(v2.toArray())
    denom = (np.linalg.norm(v1) * np.linalg.norm(v2))
    return float(np.dot(v1, v2) / denom) if denom != 0 else 0.0

# -------------------------
# STEP 6: Runtime Function for User + Current SKU
# -------------------------
def get_recommendations(user_id, current_sku, top_n=30):
    # Current SKU vector
    current_vec = df_features.filter(col("skuCode") == current_sku).select("finalFeatures").collect()
    if not current_vec:
        raise ValueError(f"SKU {current_sku} not found in df_category.")
    current_vec = current_vec[0][0]

    # Define UDF with current vector fixed
    cosine_udf = udf(lambda v: cosine_similarity(current_vec, v), DoubleType())

    # ALS recs for this user
    als_recs_user = als_enriched.filter(col("user_id") == user_id).limit(top_n)

    # Join ALS recs with feature vectors
    als_with_features = als_recs_user.join(df_features, on="skuCode", how="left")

    # Compute similarity
    als_with_similarity = als_with_features.withColumn("cosine_similarity", cosine_udf(col("finalFeatures")))

    # Final output
    return als_with_similarity.select("skuCode", "als_score", "cosine_similarity").orderBy(col("cosine_similarity").desc())

# -------------------------
# STEP 7: Example Run
# -------------------------
user_id = "USER123"      # runtime user
current_sku = "SKU12345" # runtime SKU intent

result = get_recommendations(user_id, current_sku, top_n=30)
result.show(30, truncate=False)
