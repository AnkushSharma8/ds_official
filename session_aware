Perfect âœ… thanks for clarifying â€” that makes sense.
Weâ€™ll keep df_final strictly as historical interactions (training) and df_category as your product catalog lookup (used only at runtime for intent and candidate features).

Hereâ€™s a cleaned-up version of the recommender code with your separation in mind:

ðŸ”¹ Full Python Code (Two Separate Datasets)
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime

# =========================
# Helpers
# =========================
def _ensure_datetime(df, col="event_date"):
    """Ensure event_date is pandas datetime (original format yyyymmdd)."""
    if np.issubdtype(df[col].dtype, np.number):
        df[col] = df[col].astype(int).astype(str)
    df[col] = pd.to_datetime(df[col], format="%Y%m%d", errors="coerce")
    return df

def _compute_recency_weight(df, today=None, tau_days=30):
    """Exponential decay recency weight."""
    df = df.copy()
    if today is None:
        today = pd.Timestamp(datetime.utcnow().date())
    age = (today - df["event_date"]).dt.days.clip(lower=0).fillna(0)
    df["recency_w"] = np.exp(-age / float(tau_days))
    return df

def _price_band_filter(df, price, pct=0.20):
    return df[(df["medianRsp"] >= (1 - pct) * price) & (df["medianRsp"] <= (1 + pct) * price)]

def _sku_popularity(df):
    """Popularity proxy from interactions (weighted by recency if available)."""
    w = df["recency_w"] if "recency_w" in df.columns else 1.0
    return (df.assign(w=w)
              .groupby(["sku_code"], as_index=False)
              .agg(popularity=("w", "sum")))

# =========================
# Content (description) similarity
# =========================
def build_tfidf(df_category, text_col="skuDescription"):
    texts = df_category[text_col].fillna("").astype(str).values
    tfidf = TfidfVectorizer(min_df=1, stop_words="english")
    mat = tfidf.fit_transform(texts)
    return tfidf, mat

def content_sim_for_sku(sku_code, df_category, tfidf_mat):
    idx_map = {sku: i for i, sku in enumerate(df_category["sku_code"].values)}
    if sku_code not in idx_map:
        return np.zeros(tfidf_mat.shape[0])
    i = idx_map[sku_code]
    sims = cosine_similarity(tfidf_mat[i], tfidf_mat).ravel()
    return dict(zip(df_category["sku_code"].values, sims))

# =========================
# User-User CF (recency-weighted)
# =========================
def user_user_cf_weighted(user_id, df_final):
    """Return SKU scores from top-N neighbors (recency-weighted)."""
    w = df_final["recency_w"] if "recency_w" in df_final.columns else 1.0
    buy_boost = 1.0 + 1.0 * df_final.get("purchased_status", pd.Series(0, index=df_final.index))
    df_final = df_final.assign(int_w=w * buy_boost)

    user_item = (df_final
                 .pivot_table(index="user_id", columns="sku_code", values="int_w", aggfunc="sum")
                 .fillna(0.0))

    if user_id not in user_item.index:
        return {}

    sim = cosine_similarity(user_item)
    sim_df = pd.DataFrame(sim, index=user_item.index, columns=user_item.index)

    neigh = sim_df.loc[user_id].drop(index=user_id).sort_values(ascending=False)
    neigh = neigh[neigh > 0].head(20)

    if neigh.empty:
        return {}

    neigh_mat = user_item.loc[neigh.index]
    scores = neigh.values @ (neigh_mat.values)
    scores = pd.Series(scores, index=neigh_mat.columns)

    # remove items already interacted with
    user_items = set(user_item.loc[user_id][user_item.loc[user_id] > 0].index)
    scores = scores.drop(labels=list(user_items), errors="ignore")

    # scale to [0,1]
    if len(scores) > 0:
        smin, smax = scores.min(), scores.max()
        if smax > smin:
            scores = (scores - smin) / (smax - smin)
        else:
            scores = scores * 0
    return scores.to_dict()

# =========================
# Main Recommender
# =========================
def recommend(user_id, sku_code, df_final, df_category,
              tau_days=30, price_band=0.20,
              w_cf=0.5, w_affinity=0.3, w_text=0.15, w_pop=0.05,
              topn=10):
    """
    Hybrid recommender:
    - If user exists & history matches intent â†’ CF
    - Else if user exists â†’ intent-based (affinity + text + popularity)
    - Else (cold start) â†’ popularity + text
    """
    # Ensure recency weights
    if "event_date" in df_final.columns:
        df_final = _ensure_datetime(df_final.copy(), "event_date")
        df_final = _compute_recency_weight(df_final, tau_days=tau_days)
    else:
        df_final = df_final.copy()

    # Intent from catalog
    sku_row = df_category[df_category["sku_code"] == sku_code]
    if sku_row.empty:
        raise ValueError(f"sku_code {sku_code} not found in df_category")
    sku_row = sku_row.iloc[0]
    intent_cat, intent_brand, intent_price = sku_row["categoryName"], sku_row["brandName"], sku_row["medianRsp"]

    # TF-IDF (once per catalog)
    _, tfidf_mat = build_tfidf(df_category, text_col="skuDescription")
    text_sim_map = content_sim_for_sku(sku_code, df_category, tfidf_mat)

    # Routing
    user_exists = user_id in set(df_final["user_id"].unique())
    if user_exists:
        user_hist = df_final[df_final["user_id"] == user_id]

        # Does history overlap with current intent?
        hist_match = user_hist[
            (user_hist["categoryName"] == intent_cat) |
            (user_hist["brandName"] == intent_brand) |
            ((user_hist["medianRsp"] - intent_price).abs() <= price_band * intent_price)
        ]

        if not hist_match.empty:
            # ========== CF path ==========
            cf_scores = user_user_cf_weighted(user_id, df_final)

            candidates = df_category[df_category["categoryName"] == intent_cat].copy()
            candidates = _price_band_filter(candidates, intent_price, pct=price_band)

            candidates["score_cf"] = candidates["sku_code"].map(cf_scores).fillna(0.0)
            candidates["score_aff"] = (0.6 * candidates.get("brand_affinity", 0.0) +
                                       0.4 * candidates.get("category_affinity", 0.0))
            candidates["score_text"] = candidates["sku_code"].map(text_sim_map).fillna(0.0)

            pop = _sku_popularity(df_final)
            candidates = candidates.merge(pop, on="sku_code", how="left").fillna({"popularity": 0.0})
            if candidates["popularity"].max() > candidates["popularity"].min():
                candidates["score_pop"] = (candidates["popularity"] - candidates["popularity"].min()) / \
                                          (candidates["popularity"].max() - candidates["popularity"].min())
            else:
                candidates["score_pop"] = 0.0

            candidates["score"] = (w_cf * candidates["score_cf"] +
                                   w_affinity * candidates["score_aff"] +
                                   w_text * candidates["score_text"] +
                                   w_pop * candidates["score_pop"])

            candidates = candidates[candidates["sku_code"] != sku_code]

            return (candidates.sort_values("score", ascending=False)
                              .head(topn)
                              [["sku_code","skuDescription","brandName","categoryName","medianRsp","score"]]
                              .reset_index(drop=True))

        else:
            # ========== Intent-based path ==========
            candidates = df_category[df_category["categoryName"] == intent_cat].copy()
            candidates = _price_band_filter(candidates, intent_price, pct=price_band)

            # use userâ€™s average affinity if available
            u_brand_aff = user_hist["brand_affinity"].mean() if "brand_affinity" in user_hist.columns else 0.0
            u_cat_aff = user_hist["category_affinity"].mean() if "category_affinity" in user_hist.columns else 0.0
            candidates["score_aff"] = 0.6 * u_brand_aff + 0.4 * u_cat_aff

            candidates["score_text"] = candidates["sku_code"].map(text_sim_map).fillna(0.0)

            pop = _sku_popularity(df_final)
            candidates = candidates.merge(pop, on="sku_code", how="left").fillna({"popularity": 0.0})
            if candidates["popularity"].max() > candidates["popularity"].min():
                candidates["score_pop"] = (candidates["popularity"] - candidates["popularity"].min()) / \
                                          (candidates["popularity"].max() - candidates["popularity"].min())
            else:
                candidates["score_pop"] = 0.0

            candidates["score"] = (w_affinity * candidates["score_aff"] +
                                   w_text * candidates["score_text"] +
                                   w_pop * candidates["score_pop"])

            candidates = candidates[candidates["sku_code"] != sku_code]

            return (candidates.sort_values("score", ascending=False)
                              .head(topn)
                              [["sku_code","skuDescription","brandName","categoryName","medianRsp","score"]]
                              .reset_index(drop=True))
    else:
        # ========== Cold start ==========
        pool = df_final[df_final["categoryName"] == intent_cat]
        if pool.empty:
            candidates = df_category[df_category["categoryName"] == intent_cat].copy()
            candidates["popularity"] = 0.0
        else:
            pop = _sku_popularity(pool).sort_values("popularity", ascending=False)
            candidates = df_category.merge(pop, on="sku_code", how="inner")

        candidates = _price_band_filter(candidates, intent_price, pct=price_band)
        candidates["score_text"] = candidates["sku_code"].map(text_sim_map).fillna(0.0)

        if "popularity" not in candidates.columns:
            candidates["popularity"] = 0.0
        if candidates["popularity"].max() > candidates["popularity"].min():
            candidates["score_pop"] = (candidates["popularity"] - candidates["popularity"].min()) / \
                                      (candidates["popularity"].max() - candidates["popularity"].min())
        else:
            candidates["score_pop"] = 0.0

        candidates["score"] = (0.75 * candidates["score_pop"] + 0.25 * candidates["score_text"])
        candidates = candidates[candidates["sku_code"] != sku_code]

        return (candidates.sort_values("score", ascending=False)
                          .head(topn)
                          [["sku_code","skuDescription","brandName","categoryName","medianRsp","score"]]
                          .reset_index(drop=True))

ðŸ”¹ Key Adjustments for Your Case

No merging: df_final (history) and df_category (catalog) are kept separate.

At runtime: You pass user_id + sku_code, look up intent in df_category, and then use df_final only for collaborative filtering, affinities, and popularity.

TF-IDF is built only on df_category descriptions (never touching df_final).

Popularity is computed from df_final (actual usage data).

ðŸ‘‰ Do you want me to also show you how to precompute CF similarity matrix and TF-IDF embeddings once (offline), so at runtime you only do lightweight scoring instead of rebuilding every time? Thatâ€™s usually whatâ€™s done in production.
