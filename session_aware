Thought for 6s

Awesome—here’s a complete, production-ready Python script that builds a Session-Aware User-to-User Hybrid Collaborative Filtering recommender using every column you listed, including Event Date (recency weighting) and Sku Description (TF-IDF content similarity).

Paste this into a file named session_aware_user_user_recommender.py and run it.

# session_aware_user_user_recommender.py
# -------------------------------------------------------------
# Session-Aware User-to-User Hybrid Collaborative Filtering
# - Uses user-user CF (cosine on time-decayed purchases)
# - Session-aware filtering (Category / SubCategory / Brand / Price)
# - Date-based time decay to prioritize recent behavior
# - TF-IDF text embeddings for SKU descriptions (content similarity)
# - Handles: history aligned, history ≠ intent, no history
# - Output: ranked list of SqCodes
# -------------------------------------------------------------

from __future__ import annotations
import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional, List
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# -----------------------------
# Utility helpers
# -----------------------------
def _ensure_datetime(df: pd.DataFrame, col: str) -> pd.DataFrame:
    if not np.issubdtype(df[col].dtype, np.datetime64):
        df[col] = pd.to_datetime(df[col], errors="coerce")
    return df

def _time_decay_weights(deltas_days: pd.Series, half_life_days: float = 30.0) -> pd.Series:
    """
    Exponential time decay. Half-life = days until weight halves.
    w = 0.5 ** (days / half_life)
    """
    half_life = max(1e-6, half_life_days)
    return 0.5 ** (deltas_days / half_life)

# -----------------------------
# Recommender
# -----------------------------
class SessionAwareUserUserRecommender:
    """
    Expected columns in df:
      UserID, EventDate, SqCode, SqQuantity, Purchased_Status (0/1),
      CategoryName, SubCategoryName, MaterialGroupName, BrandName,
      SqDescription, Price, BrandAffinity, CategoryAffinity
    """

    def __init__(
        self,
        df: pd.DataFrame,
        half_life_days: float = 30.0,
        max_neighbors: int = 20,
        neighbor_item_cap: int = 2000,
        min_desc_len: int = 2,
    ):
        self.df = df.copy()
        self.half_life_days = half_life_days
        self.max_neighbors = max_neighbors
        self.neighbor_item_cap = neighbor_item_cap
        self.min_desc_len = min_desc_len

        # Filled in during preprocess()
        self.user_item_matrix = None              # users x items (time-decayed purchase strength)
        self.user_similarity = None               # users x users cosine similarity
        self.desc_vectorizer: Optional[TfidfVectorizer] = None
        self.sku_desc_matrix = None              # items x tfidf
        self.sku_index = None                    # mapping SqCode -> row index in sku_desc_matrix
        self.user_brand_aff = None               # user -> avg BrandAffinity
        self.user_cat_aff = None                 # user -> avg CategoryAffinity
        self.item_meta = None                    # per-SKU metadata (Category/Brand/Price etc.)

    # -------------------------
    # Preprocessing
    # -------------------------
    def preprocess(self) -> None:
        df = self.df

        # 1) Ensure datetime and compute recency deltas
        if "EventDate" not in df.columns:
            raise ValueError("EventDate column is required")
        df = _ensure_datetime(df, "EventDate")
        max_date = df["EventDate"].max()
        df["days_since"] = (max_date - df["EventDate"]).dt.days.clip(lower=0)

        # 2) Time-decayed interaction strength (purchase=1, non-purchase=0 contributes nothing)
        #    You can optionally add softer signal from views if present; here we use purchases only.
        df["strength"] = df["Purchased_Status"].astype(float) * _time_decay_weights(
            df["days_since"], self.half_life_days
        )

        # 3) Build user-item matrix (using max to collapse multiple events per user/SKU)
        uim = (
            df.pivot_table(
                index="UserID",
                columns="SqCode",
                values="strength",
                aggfunc="max",
                fill_value=0.0,
            )
            .astype(float)
        )
        self.user_item_matrix = uim

        # 4) User-user cosine similarity
        if uim.shape[0] >= 2:
            self.user_similarity = pd.DataFrame(
                cosine_similarity(uim),
                index=uim.index,
                columns=uim.index,
            )
        else:
            # Not enough users; similarity will be None
            self.user_similarity = None

        # 5) Per-SKU metadata for runtime filters
        keep_cols = [
            "SqCode", "CategoryName", "SubCategoryName", "MaterialGroupName",
            "BrandName", "Price", "SqDescription"
        ]
        self.item_meta = (
            df[keep_cols].drop_duplicates("SqCode").set_index("SqCode")
        )

        # 6) User-level affinities (average over rows)
        self.user_brand_aff = (
            df.groupby("UserID")["BrandAffinity"].mean().to_dict()
            if "BrandAffinity" in df.columns else {}
        )
        self.user_cat_aff = (
            df.groupby("UserID")["CategoryAffinity"].mean().to_dict()
            if "CategoryAffinity" in df.columns else {}
        )

        # 7) TF-IDF over SKU descriptions
        desc_series = self.item_meta["SqDescription"].fillna("").astype(str).str.strip()
        desc_series = desc_series.where(desc_series.str.split().str.len().fillna(0) >= self.min_desc_len, "")
        self.desc_vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words="english",
            ngram_range=(1, 2),
            min_df=1
        )
        self.sku_desc_matrix = self.desc_vectorizer.fit_transform(desc_series.values)
        # map SqCode -> row index
        self.sku_index = {sku: i for i, sku in enumerate(self.item_meta.index)}

    # -------------------------
    # Public API
    # -------------------------
    def recommend(
        self,
        user_id: str | int,
        session_intent: Dict,
        top_n: int = 10,
    ) -> List[str]:
        """
        session_intent example:
          {
            "CategoryName": "Chocolate",
            "SubCategoryName": "Dark",
            "BrandName": "Cadbury",
            "PriceRange": (150, 300),
            # Optional if you have a query text from search box or PDP:
            # "QueryText": "almond dark chocolate"
            # Optional if you know the currently viewed SKU (PDP):
            # "CurrentSqCode": "B123"
          }
        """
        if self.user_item_matrix is None:
            raise RuntimeError("Call preprocess() before recommend().")

        # Identify user history (purchased items only)
        user_hist_items = set()
        if user_id in self.user_item_matrix.index:
            row = self.user_item_matrix.loc[user_id]
            user_hist_items = set(row[row > 0].index)

        has_history = len(user_hist_items) > 0

        # Derive "history aligned?" by checking overlap in category/brand with history
        history_categories = set()
        history_brands = set()
        if has_history:
            meta = self.item_meta.loc[list(user_hist_items)] if user_hist_items else pd.DataFrame()
            if not meta.empty:
                history_categories = set(meta["CategoryName"].dropna().unique())
                history_brands = set(meta["BrandName"].dropna().unique())

        intent_cat = session_intent.get("CategoryName")
        intent_brand = session_intent.get("BrandName")

        history_aligned = (
            (intent_cat is not None and intent_cat in history_categories) or
            (intent_brand is not None and intent_brand in history_brands)
        )

        if not has_history:
            # Case 3: No history → session + cohort popularity
            return self._recommend_no_history(session_intent, top_n)

        if history_aligned:
            # Case 1: History aligned with intent
            return self._recommend_history_aligned(user_id, session_intent, user_hist_items, top_n)
        else:
            # Case 2: History ≠ intent → session filters first, then rank with personalization
            return self._recommend_history_mismatch(user_id, session_intent, top_n)

    # -------------------------
    # Case 1: History aligned with intent
    # -------------------------
    def _recommend_history_aligned(
        self,
        user_id: str | int,
        session_intent: Dict,
        user_hist_items: set,
        top_n: int
    ) -> List[str]:

        # 1) Collect neighbors
        neighbor_items = self._neighbor_items(user_id, cap=self.neighbor_item_cap)
        # Remove already purchased
        neighbor_items = neighbor_items.difference(user_hist_items)

        # 2) Apply session intent filters (Category, SubCategory, Brand, Price)
        filtered = self._apply_intent_filters(list(neighbor_items), session_intent)

        if not filtered:
            # Fallback to popularity within intent
            return self._popular_within_intent(session_intent, top_n)

        # 3) Rank candidates: blend
        #    - Neighbor support (how many neighbors own it)
        #    - Price closeness to target range
        #    - Content similarity to user history centroid (descriptions)
        scores = self._score_candidates_history_aligned(user_id, filtered, session_intent)

        return [sq for sq, _ in scores[:top_n]]

    def _score_candidates_history_aligned(
        self,
        user_id: str | int,
        candidates: List[str],
        session_intent: Dict
    ) -> List[Tuple[str, float]]:
        # Neighbor support
        neighbor_list = self._top_neighbors(user_id)
        neighbor_sets = {nbr: self._items_of(nbr) for nbr in neighbor_list}
        neighbor_support = []
        for sq in candidates:
            cnt = sum(1 for s in neighbor_sets.values() if sq in s)
            neighbor_support.append(cnt)

        neighbor_support = np.array(neighbor_support, dtype=float)

        # Price closeness (0..1)
        price_scores = self._price_alignment_scores(candidates, session_intent.get("PriceRange"))

        # Content similarity: candidate vs. user's history centroid embedding
        hist_centroid = self._user_history_desc_centroid(user_id)
        if hist_centroid is not None:
            cand_mat = self._sku_vecs(candidates)
            desc_sims = cosine_similarity(cand_mat, hist_centroid.reshape(1, -1)).ravel()
        else:
            desc_sims = np.zeros(len(candidates))

        # Normalize components
        def _nz_std(a):
            s = a.std()
            return s if s > 1e-9 else 1.0
        ns = (neighbor_support - neighbor_support.mean()) / _nz_std(neighbor_support)
        ps = (price_scores - price_scores.mean()) / _nz_std(price_scores)
        ds = (desc_sims - desc_sims.mean()) / _nz_std(desc_sims)

        # Weighted sum (tuneable)
        score = 0.5 * ns + 0.3 * ps + 0.2 * ds

        ranked = sorted(zip(candidates, score), key=lambda x: x[1], reverse=True)
        return ranked

    # -------------------------
    # Case 2: History ≠ intent
    # -------------------------
    def _recommend_history_mismatch(
        self,
        user_id: str | int,
        session_intent: Dict,
        top_n: int
    ) -> List[str]:
        # 1) Session-first candidates (filters)
        candidates = self._apply_intent_filters(self.item_meta.index.tolist(), session_intent)
        if not candidates:
            return self._popular_within_intent(session_intent, top_n)

        # 2) Rank: personalization using user-level brand/category affinity
        user_brand_aff = self.user_brand_aff.get(user_id, 0.5)
        user_cat_aff = self.user_cat_aff.get(user_id, 0.5)

        # Price alignment + description similarity to intent “query”
        price_scores = self._price_alignment_scores(candidates, session_intent.get("PriceRange"))
        intent_vec = self._intent_text_centroid(session_intent)  # derived from QueryText or category/brand centroids
        if intent_vec is not None:
            cand_mat = self._sku_vecs(candidates)
            desc_sims = cosine_similarity(cand_mat, intent_vec.reshape(1, -1)).ravel()
        else:
            desc_sims = np.zeros(len(candidates))

        # Affinity boost if candidate brand/category matches user's historic bias
        brand_match = np.array([
            1.0 if self.item_meta.loc[c, "BrandName"] == session_intent.get("BrandName") else 0.0
            for c in candidates
        ])
        cat_match = np.array([
            1.0 if self.item_meta.loc[c, "CategoryName"] == session_intent.get("CategoryName") else 0.0
            for c in candidates
        ])

        # Weighted rank (tune as needed)
        score = (
            0.45 * price_scores +
            0.30 * desc_sims +
            0.15 * brand_match * user_brand_aff +
            0.10 * cat_match * user_cat_aff
        )

        ranked = [c for c, _ in sorted(zip(candidates, score), key=lambda x: x[1], reverse=True)]
        return ranked[:top_n]

    # -------------------------
    # Case 3: No history
    # -------------------------
    def _recommend_no_history(
        self,
        session_intent: Dict,
        top_n: int
    ) -> List[str]:
        # Session filters first
        candidates = self._apply_intent_filters(self.item_meta.index.tolist(), session_intent)
        if not candidates:
            # Global popularity fallback
            return self._global_popularity(top_n)

        # Cohort popularity: most purchased in this intent slice
        return self._popular_within_intent(session_intent, top_n)

    # -------------------------
    # Building blocks
    # -------------------------
    def _top_neighbors(self, user_id: str | int) -> List[str | int]:
        if self.user_similarity is None or user_id not in self.user_similarity.index:
            return []
        sims = self.user_similarity.loc[user_id].drop(index=user_id, errors="ignore")
        return sims.sort_values(ascending=False).head(self.max_neighbors).index.tolist()

    def _items_of(self, user_id: str | int) -> set:
        if user_id not in self.user_item_matrix.index:
            return set()
        row = self.user_item_matrix.loc[user_id]
        return set(row[row > 0].index)

    def _neighbor_items(self, user_id: str | int, cap: int = 2000) -> set:
        neighbors = self._top_neighbors(user_id)
        items = set()
        for nbr in neighbors:
            items |= self._items_of(nbr)
            if len(items) >= cap:
                break
        return items

    def _apply_intent_filters(self, sq_list: List[str], session_intent: Dict) -> List[str]:
        if not sq_list:
            return []
        meta = self.item_meta.loc[sq_list]

        def apply_eq(col):
            val = session_intent.get(col)
            return meta[col] == val if val is not None else pd.Series([True] * len(meta), index=meta.index)

        m_cat = apply_eq("CategoryName")
        m_sub = apply_eq("SubCategoryName")
        m_brand = apply_eq("BrandName")

        m_price = pd.Series([True] * len(meta), index=meta.index)
        pr = session_intent.get("PriceRange")
        if isinstance(pr, tuple) and len(pr) == 2:
            low, high = pr
            m_price = (meta["Price"] >= low) & (meta["Price"] <= high)

        filtered = meta[m_cat & m_sub & m_brand & m_price].index.tolist()
        return filtered

    def _price_alignment_scores(self, candidates: List[str], price_range: Optional[Tuple[float, float]]) -> np.ndarray:
        if not candidates:
            return np.array([])
        if not price_range or len(price_range) != 2:
            # Neutral if no price range
            return np.ones(len(candidates), dtype=float)

        low, high = price_range
        target = (low + high) / 2.0
        width = max(1.0, (high - low) / 2.0)
        prices = self.item_meta.loc[candidates, "Price"].astype(float).values
        # Gaussian-like closeness
        score = np.exp(-((prices - target) ** 2) / (2 * (0.5 * width) ** 2))
        # Normalize 0..1
        if score.max() > 0:
            score = score / score.max()
        return score

    def _sku_vecs(self, sq_list: List[str]):
        idxs = [self.sku_index[sq] for sq in sq_list]
        return self.sku_desc_matrix[idxs, :]

    def _user_history_desc_centroid(self, user_id: str | int):
        items = list(self._items_of(user_id))
        items = [i for i in items if i in self.sku_index]
        if not items:
            return None
        mat = self._sku_vecs(items)
        centroid = np.asarray(mat.mean(axis=0)).ravel()
        return centroid if np.linalg.norm(centroid) > 0 else None

    def _intent_text_centroid(self, session_intent: Dict):
        """
        Build a text vector for intent:
          - If QueryText provided, use it.
          - Else average TF-IDF of all SKUs in the requested Category/Brand slice.
        """
        # 1) If QueryText available
        q = session_intent.get("QueryText")
        if q and self.desc_vectorizer is not None:
            vec = self.desc_vectorizer.transform([q])
            arr = np.asarray(vec.A).ravel()
            return arr if np.linalg.norm(arr) > 0 else None

        # 2) Else, centroid of descriptions within filters
        sqs = self._apply_intent_filters(self.item_meta.index.tolist(), session_intent)
        sqs = [s for s in sqs if s in self.sku_index]
        if not sqs:
            return None
        mat = self._sku_vecs(sqs)
        centroid = np.asarray(mat.mean(axis=0)).ravel()
        return centroid if np.linalg.norm(centroid) > 0 else None

    def _popular_within_intent(self, session_intent: Dict, top_n: int) -> List[str]:
        # Popular = count of Purchased_Status==1 within filtered slice
        mask = pd.Series([True] * len(self.df))
        for k in ["CategoryName", "SubCategoryName", "BrandName"]:
            v = session_intent.get(k)
            if v is not None:
                mask &= (self.df[k] == v)

        pr = session_intent.get("PriceRange")
        if isinstance(pr, tuple) and len(pr) == 2:
            low, high = pr
            mask &= (self.df["Price"] >= low) & (self.df["Price"] <= high)

        sub = self.df[mask & (self.df["Purchased_Status"] == 1)]
        if sub.empty:
            return self._global_popularity(top_n)

        counts = sub.groupby("SqCode")["Purchased_Status"].sum().sort_values(ascending=False)
        return counts.head(top_n).index.tolist()

    def _global_popularity(self, top_n: int) -> List[str]:
        counts = (
            self.df[self.df["Purchased_Status"] == 1]
            .groupby("SqCode")["Purchased_Status"]
            .sum()
            .sort_values(ascending=False)
        )
        return counts.head(top_n).index.tolist()


# -------------------------------------------------------------
# Example usage (replace with your real CSV and columns)
# -------------------------------------------------------------
if __name__ == "__main__":
    # Example toy data; replace with your dataset
    data = {
        'UserID': [1,1,2,2,3,3,4],
        'EventDate': [
            "2025-07-20","2025-08-10","2025-07-18","2025-08-05","2025-08-07","2025-08-12","2025-08-15"
        ],
        'SqCode': ['A','B','A','C','D','B','E'],
        'SqQuantity': [1,2,1,1,1,2,1],
        'Purchased_Status': [1,1,1,0,1,1,0],
        'CategoryName': ['Liquor','Chocolate','Liquor','Snacks','Chocolate','Chocolate','Liquor'],
        'SubCategoryName': ['Vodka','Dark','Vodka','Chips','Milk','Dark','Whiskey'],
        'MaterialGroupName': ['Liquor','Food','Liquor','Food','Food','Food','Liquor'],
        'BrandName': ['Absolut','Cadbury','Smirnoff','Lays','Cadbury','Nestle','Jack Daniels'],
        'SqDescription': [
            "Premium clear vodka from Sweden",
            "Silky smooth dark chocolate bar",
            "Classic Russian style vodka",
            "Crunchy salted potato chips",
            "Creamy milk chocolate with nuts",
            "Rich dark cocoa chocolate",
            "Aged Tennessee whiskey bottle"
        ],
        'Price': [2000,200,1800,50,220,210,2500],
        'BrandAffinity': [0.9,0.7,0.8,0.4,0.85,0.75,0.6],
        'CategoryAffinity': [0.95,0.65,0.9,0.5,0.88,0.72,0.7]
    }
    df = pd.DataFrame(data)

    # If using a CSV:
    # df = pd.read_csv("your_data.csv")

    model = SessionAwareUserUserRecommender(df, half_life_days=30)
    model.preprocess()

    # ---- Case 1: History aligned with intent
    intent1 = {
        "CategoryName": "Liquor",
        "BrandName": "Absolut",
        "PriceRange": (1500, 2400)
    }
    print("Case 1 (history aligned) →", model.recommend(user_id=1, session_intent=intent1, top_n=5))

    # ---- Case 2: History ≠ intent (user bought liquor, now browsing chocolate)
    intent2 = {
        "CategoryName": "Chocolate",
        "SubCategoryName": "Dark",
        "BrandName": "Cadbury",
        "PriceRange": (150, 300),
        # Optional: "QueryText": "silky dark chocolate almond"
    }
    print("Case 2 (history ≠ intent) →", model.recommend(user_id=2, session_intent=intent2, top_n=5))

    # ---- Case 3: No history (user 99)
    intent3 = {
        "CategoryName": "Chocolate",
        "PriceRange": (150, 300)
    }
    print("Case 3 (no history) →", model.recommend(user_id=99, session_intent=intent3, top_n=5))

What this code does (quickly)

Date-aware: Uses exponential time decay so recent purchases carry more weight in user similarity.

CF core: Builds a user–item matrix from time-decayed purchase strength and computes user–user cosine similarity.

Session-aware: At runtime, filters candidates by Category/SubCategory/Brand/Price.

Description-aware: Uses TF-IDF on SqDescription to score semantic similarity (great when history ≠ intent and for cold-ish items).

Three cases:

History aligned → neighbors’ items filtered by intent, then ranked by neighbor support + price closeness + description similarity to user history.

History ≠ intent → session filters first, then rank by price closeness + TF-IDF similarity to intent text/centroid + (brand/category match × user affinities).

No history → session filters + cohort popularity in that slice (falls back to global popularity if needed).

Output: returns SqCode list ranked by likelihood.

If you want, I can add:

A/B tunable weights via config,

Safeguards for sparse data,

Incremental updates (refresh similarity nightly + micro-batch deltas).
